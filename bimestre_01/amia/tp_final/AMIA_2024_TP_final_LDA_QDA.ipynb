{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autores: \n",
    "* Mauro Barquinero\n",
    "* Yandri Uchuari\n",
    "* Marck Murillo\n",
    "* Matias Tripode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpJ7s_SIVu_I"
   },
   "source": [
    "# Trabajo Práctico Final: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n",
    "\n",
    "### Definición: Clasificador Bayesiano\n",
    "\n",
    "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
    "\n",
    "De esta manera dicha probabilidad *a posteriori* resulta\n",
    "$$\n",
    "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
    "$$\n",
    "\n",
    "La regla de decisión de Bayes es entonces\n",
    "$$\n",
    "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
    "$$\n",
    "\n",
    "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
    "\n",
    "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
    "\n",
    "### Distribución condicional\n",
    "\n",
    "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
    "\n",
    "Por definición, se tiene entonces que para una clase $j$:\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
    "$$\n",
    "\n",
    "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
    "\n",
    "### LDA\n",
    "\n",
    "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "### Entrenamiento/Ajuste\n",
    "\n",
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
    "\n",
    "### Predicción\n",
    "\n",
    "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TDWOgpJWKQa"
   },
   "source": [
    "## Estructura del código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yEV8WbiWl6k"
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teF9O9JJmG7Z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det, inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDBLvbTtlwzs"
   },
   "outputs": [],
   "source": [
    "class ClassEncoder:\n",
    "  def fit(self, y):\n",
    "    self.names = np.unique(y)\n",
    "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
    "    self.fmt = y.dtype\n",
    "    # Q1: por que no hace falta definir un class_to_name para el mapeo inverso?\n",
    "\n",
    "  def _map_reshape(self, f, arr):\n",
    "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
    "    # Q2: por que hace falta un reshape?\n",
    "\n",
    "  def transform(self, y):\n",
    "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
    "\n",
    "  def fit_transform(self, y):\n",
    "    self.fit(y)\n",
    "    return self.transform(y)\n",
    "\n",
    "  def detransform(self, y_hat):\n",
    "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0KYC8_uSOu4"
   },
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    self.encoder = ClassEncoder()\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    # Q3: para que sirve bincount?\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # first encode the classes\n",
    "    y = self.encoder.fit_transform(y)\n",
    "\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # check that a_priori has the correct number of classes\n",
    "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
    "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRamFdiGDuSR"
   },
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRtC9HEkO5Hu"
   },
   "outputs": [],
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
    "\n",
    "        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KS_zoK-gWkRf"
   },
   "source": [
    "## Código para pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz19b6NJed2A"
   },
   "source": [
    "Seteamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m05KrhUDINVs"
   },
   "outputs": [],
   "source": [
    "# hiperparámetros\n",
    "rng_seed = 6543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hkXcoldXOqs",
    "outputId": "2ce8d627-3433-4bdd-d370-85f6b703a7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (150, 4), Y:(150, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "def get_penguins():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
    "\n",
    "# showing for iris\n",
    "X_full, y_full = get_iris_dataset()\n",
    "\n",
    "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAk-UQCjKecT",
    "outputId": "9566d67a-b78b-4809-bb94-8f605b065db6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek data matrix\n",
    "X_full[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdzMURX2KVdO",
    "outputId": "af5fc3ac-b391-4769-de47-44cea4f566c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa'],\n",
       "       ['setosa']], dtype='<U10')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek target vector\n",
    "y_full[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl8UFh1OegbJ"
   },
   "source": [
    "Separamos el dataset en train y test para medir performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKP_QmWCIECs",
    "outputId": "07798c6a-aa54-430e-d46d-becc2a4315ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 90) (1, 90) (4, 60) (1, 60)\n"
     ]
    }
   ],
   "source": [
    "# preparing data, train - test validation\n",
    "# 70-30 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_transpose(X, y, test_sz, random_state):\n",
    "    # split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=random_state)\n",
    "\n",
    "    # transpose so observations are column vectors\n",
    "    return X_train.T, y_train.T, X_test.T, y_test.T\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  return (y_true == y_pred).mean()\n",
    "\n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwgXFPbJemb_"
   },
   "source": [
    "Entrenamos un QDA y medimos su accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGIf2TA5SpoT"
   },
   "outputs": [],
   "source": [
    "qda = QDA()\n",
    "\n",
    "qda.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0Q30DyLWpTL",
    "outputId": "dbccae86-840c-412f-ed97-22cfac21238a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train (apparent) error is 0.0111 while test error is 0.0167\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy(train_y, qda.predict(train_x))\n",
    "test_acc = accuracy(test_y, qda.predict(test_x))\n",
    "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QcLtNNIevC_"
   },
   "source": [
    "Con el magic %%timeit podemos estimar el tiempo que tarda en correr una celda en base a varias ejecuciones. Por poner un ejemplo, acá vamos a estimar lo que tarda un ciclo completo de QDA y también su inferencia (predicción).\n",
    "\n",
    "Ojo! a veces [puede ser necesario ejecutarlo varias veces](https://stackoverflow.com/questions/10994405/python-timeit-results-cached-instead-of-calculated) para obtener resultados consistentes.\n",
    "\n",
    "Si quieren explorar otros métodos de medición también es válido!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnZT-HN2fUuW",
    "outputId": "2618e7c1-7a77-4285-bafb-c2880ad167a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.27 ms ± 55.7 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "qda.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjFbVSqfeHUX",
    "outputId": "0254a727-a1d5-4be3-b73a-2f55d2c84a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.33 ms ± 14.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "model = QDA()\n",
    "model.fit(train_x, train_y)\n",
    "model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsDYvwFEEKV5"
   },
   "source": [
    "# Consigna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Yb1V7_yXRfO"
   },
   "source": [
    "## Implementación base\n",
    "1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n",
    "    1. Uniforme (cada clase tiene probabilidad 1/3)\n",
    "    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
    "2. Repetir el punto anterior para el dataset *penguin*.\n",
    "3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA (no múltiples prioris) ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
    "4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?\n",
    "5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?\n",
    "\n",
    "\n",
    "**Sugerencia:** puede resultar de utilidad para cada inciso de comparación utilizar tablas del siguiente estilo:\n",
    "\n",
    "<center>\n",
    "\n",
    "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---:\n",
    "QDA | Iris | 125 | 0.55 | 0.85\n",
    "LDA | Iris | 125 | 0.22 | 0.8\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M32U5xI-gIv"
   },
   "source": [
    "## Optimización matemática\n",
    "\n",
    "**Sugerencia:** considerar combinaciones adecuadas de `transpose`, `reshape` y, ocasionalmente, `flatten`. Explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
    "\n",
    "### QDA\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para *n* observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de *n x n* en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "1. Implementar el modelo `FasterQDA` (se recomienda heredarlo de TensorizedQDA) de manera de eliminar el ciclo for en el método predict.\n",
    "2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`.\n",
    "3. Mostrar (puede ser con un print) dónde aparece la mencionada matriz de *n x n*, donde *n* es la cantidad de observaciones a predecir.\n",
    "4. Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n",
    "5.Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente. ¿Hay cambios en los tiempos de predicción?\n",
    "\n",
    "\n",
    "### LDA\n",
    "\n",
    "1. \"Tensorizar\" el modelo LDA y comparar sus tiempos de predicción con el modelo antes implementado. *Notar que, en modo tensorizado, se puede directamente precomputar $\\mu^T \\cdot \\Sigma^{-1} \\in \\mathbb{R}^{k \\times 1 \\times p}$ y guardar eso en vez de $\\Sigma^{-1}$.*\n",
    "2. LDA no sufre del problema antes descrito de QDA debido a que no computa productos internos, por lo que no tiene un verdadero costo extra en memoria predecir \"en batch\". Implementar el modelo `FasterLDA` y comparar sus tiempos de predicción con las versiones anteriores de LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7oPeQ0z-pux"
   },
   "source": [
    "## Preguntas teóricas\n",
    "\n",
    "1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "Mostrar los pasos por los cuales se llega a dicha expresión.\n",
    "2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n",
    "3. La implementación de QDA estima la probabilidad condicional utilizando `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x` que no es *exactamente* lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n",
    "\n",
    "El espíritu de esta componente práctica es la de establecer un mínimo de trabajo aceptable para su entrega; se invita al alumno a explorar otros aspectos que generen curiosidad, sin sentirse de ninguna manera limitado por la consigna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpOoxE2d-mOY"
   },
   "source": [
    "## Ejercicio teórico\n",
    "\n",
    "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
    "$$\n",
    "w^{(1)} =\n",
    "\\begin{pmatrix}\n",
    "0.1 & -0.5 \\\\\n",
    "-0.3 & -0.9 \\\\\n",
    "0.8 & 0.02\n",
    "\\end{pmatrix},\n",
    "b^{(1)} = \\begin{pmatrix}\n",
    "0.1 \\\\\n",
    "0.5 \\\\\n",
    "0.8\n",
    "\\end{pmatrix},\n",
    "w^{(2)} =\n",
    "\\begin{pmatrix}\n",
    "-0.4 & 0.2 & -0.5\n",
    "\\end{pmatrix},\n",
    "b^{(2)} = 0.7\n",
    "$$\n",
    "\n",
    "y donde cada capa calcula su salida vía\n",
    "\n",
    "$$\n",
    "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
    "$$\n",
    "\n",
    "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
    "\n",
    "\\\\\n",
    "Dada la observación $x=\\begin{pmatrix}\n",
    "1.8 \\\\\n",
    "-3.4\n",
    "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$.\n",
    "\n",
    "*Nota: Con una sigmoidea a la salida jamás va a poder estimar el 5 \"pedido\", pero eso no afecta al mecanismo de backpropagation!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-CNlW06-i-u"
   },
   "source": [
    "## Preguntas en el código\n",
    "Previamente las preguntas \"técnicas\" en comentarios en el código eran parte del TP, y buscaban que el alumno logre entrar en el detalle de por qué cada linea de código es como es y en el orden en el que está. Ya no forman parte de la consigna, pero se aconseja al alumno intentar responderlas. Las respuestas a las mismas se encuentran en un archivo separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "************ **Aca arrancan las respuestas y las implementaciones del TP**  ************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementacion Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CODIGO DE SETUP**\n",
    "\n",
    "El siguiente codigo de setup se utiliza para **todas las pruebas** (tanto QDA como LDA).\n",
    "Simplemente se descomentan las lineas para el dataset que queremos utilizar\n",
    "ejemplo: Si queremos utilizar el penguin quedaria\n",
    "```\n",
    "    # Cargar el dataset iris\n",
    "    # X_full, y_full = get_iris_dataset()\n",
    "    # Cargar el dataset penguin\n",
    "    X_full, y_full = get_penguins()\n",
    "```\n",
    "\n",
    "Si queremos probar sin split quedaria:\n",
    "```\n",
    "    # [Split]: Dividir el dataset entre test y train \n",
    "    # train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, test_sz, seed)\n",
    "    # [Sin Split]: Dataset completo sin split\n",
    "    train_x, train_y, test_x, test_y = X_full.T, y_full.T, X_full.T, y_full.T\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 205) (1, 205) (4, 137) (1, 137)\n"
     ]
    }
   ],
   "source": [
    "# *** CODIGO DE SETUP *** \n",
    "\n",
    "# Cargar el dataset iris\n",
    "# X_full, y_full = get_iris_dataset()\n",
    "# Cargar el dataset penguin\n",
    "X_full, y_full = get_penguins()\n",
    "\n",
    "# Seed\n",
    "seed = 16\n",
    "# Test size\n",
    "test_sz = 0.3 # 30%\n",
    "# [Split]: Dividir el dataset entre test y train \n",
    "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, test_sz, seed)\n",
    "# [Sin Split]: Dataset completo sin split\n",
    "# train_x, train_y, test_x, test_y = X_full.T, y_full.T, X_full.T, y_full.T\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QDA + Uniforme (cada clase tiene probabilidad 1/3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* QDA: Train (apparent) error is 0.0098 while test error is 0.0146\n",
      "Error (train): 0.98%\n",
      "Error (test): 1.46%\n"
     ]
    }
   ],
   "source": [
    "qda = QDA()\n",
    "qda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])\n",
    "\n",
    "y_train_pred = qda.predict(train_x)\n",
    "y_test_pred = qda.predict(test_x)\n",
    "qda_train_acc = accuracy(train_y, y_train_pred)\n",
    "qda_test_acc = accuracy(test_y, y_test_pred)\n",
    "print(f\"* QDA: Train (apparent) error is {1-qda_train_acc:.4f} while test error is {1-qda_test_acc:.4f}\")\n",
    "\n",
    "# Calculate train and test errors\n",
    "error_train = np.mean(y_train_pred != train_y)\n",
    "error_test = np.mean(y_test_pred != test_y)\n",
    "\n",
    "print(f\"Error (train): {error_train:.2%}\")\n",
    "print(f\"Error (test): {error_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QDA Primera clase con probabilidad 0.9, las demás 0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* QDA: Train (apparent) error is 0.0146 while test error is 0.0292\n",
      "Error (train): 1.46%\n",
      "Error (test): 2.92%\n"
     ]
    }
   ],
   "source": [
    "qda = QDA()\n",
    "# Primera clase con probabilidad 0.9, las demás 0.05\n",
    "qda.fit(train_x, train_y, a_priori=[0.9, 0.05,  0.05])\n",
    "\n",
    "y_train_pred = qda.predict(train_x)\n",
    "y_test_pred = qda.predict(test_x)\n",
    "qda_train_acc = accuracy(train_y, y_train_pred)\n",
    "qda_test_acc = accuracy(test_y, y_test_pred)\n",
    "print(f\"* QDA: Train (apparent) error is {1-qda_train_acc:.4f} while test error is {1-qda_test_acc:.4f}\")\n",
    "\n",
    "# Calculate train and test errors\n",
    "error_train = np.mean(y_train_pred != train_y)\n",
    "error_test = np.mean(y_test_pred != test_y)\n",
    "\n",
    "print(f\"Error (train): {error_train:.2%}\")\n",
    "print(f\"Error (test): {error_test:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QDA Segunda clase con probabilidad 0.05, 0.9 y 0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* QDA: Train (apparent) error is 0.0244 while test error is 0.0365\n",
      "Error (train): 2.44%\n",
      "Error (test): 3.65%\n"
     ]
    }
   ],
   "source": [
    "qda = QDA()\n",
    "# Segunda clase con probabilidad 0.05, 0.9 y 0.05\n",
    "qda.fit(train_x, train_y, a_priori=[0.05, 0.9,  0.05])\n",
    "\n",
    "y_train_pred = qda.predict(train_x)\n",
    "y_test_pred = qda.predict(test_x)\n",
    "qda_train_acc = accuracy(train_y, y_train_pred)\n",
    "qda_test_acc = accuracy(test_y, y_test_pred)\n",
    "print(f\"* QDA: Train (apparent) error is {1-qda_train_acc:.4f} while test error is {1-qda_test_acc:.4f}\")\n",
    "\n",
    "# Calculate train and test errors\n",
    "error_train = np.mean(y_train_pred != train_y)\n",
    "error_test = np.mean(y_test_pred != test_y)\n",
    "\n",
    "print(f\"Error (train): {error_train:.2%}\")\n",
    "print(f\"Error (test): {error_test:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QDA + Tercera clase con probabilidad 0.05, 0.05 y 0.9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* QDA: Train (apparent) error is 0.0098 while test error is 0.0146\n",
      "Error (train): 0.98%\n",
      "Error (test): 1.46%\n"
     ]
    }
   ],
   "source": [
    "qda = QDA()\n",
    "# Tercera clase con probabilidad 0.05, 0.05 y 0.9\n",
    "qda.fit(train_x, train_y, a_priori=[0.05, 0.05,  0.9])\n",
    "\n",
    "y_train_pred = qda.predict(train_x)\n",
    "y_test_pred = qda.predict(test_x)\n",
    "qda_train_acc = accuracy(train_y, y_train_pred)\n",
    "qda_test_acc = accuracy(test_y, y_test_pred)\n",
    "print(f\"* QDA: Train (apparent) error is {1-qda_train_acc:.4f} while test error is {1-qda_test_acc:.4f}\")\n",
    "\n",
    "# Calculate train and test errors\n",
    "error_train = np.mean(y_train_pred != train_y)\n",
    "error_test = np.mean(y_test_pred != test_y)\n",
    "\n",
    "print(f\"Error (train): {error_train:.2%}\")\n",
    "print(f\"Error (test): {error_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabla comparativa de QDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo | Dataset | Seed | a_priori | Split | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "QDA | Iris | 125 | [1/3, 1/3, 1/3] | 70/30 | 2.86% | 0.00%\n",
    "QDA | Iris | 125 | [0.9, 0.05, 0.05] | 70/30 | 2.86% | 0.00%\n",
    "QDA | Iris | 125 | [0.05, 0.9, 0.05] | 70/30 | 6.67% | 2.22%\n",
    "QDA | Iris | 125 | [0.05, 0.05, 0.9] | 70/30 | 4.76% | 0.00%\n",
    "QDA | Iris | 125 | [1/3, 1/3, 1/3] | 100 | 2.00% | 2.00%\n",
    "QDA | Iris | 125 | [0.9, 0.05, 0.05] | 100 | 2.00% | 2.00%\n",
    "QDA | Iris | 125 | [0.05, 0.9, 0.05] | 100 | 3.33% | 3.33%\n",
    "QDA | Iris | 125 | [0.05, 0.05, 0.9] | 100 | 4.00% | 4.00%\n",
    " |  |  |  |  |  |\n",
    "QDA | Penguin | 125 | [1/3, 1/3, 1/3] | 70/30 | 0.84% | 0.97%\n",
    "QDA | Penguin | 125 | [0.9, 0.05, 0.05] | 70/30 | 1.67% | 1.94%\n",
    "QDA | Penguin | 125 | [0.05, 0.9, 0.05] | 70/30 | 4.60% | 6.80%\n",
    "QDA | Penguin | 125 | [0.05, 0.05, 0.9] | 70/30 | 0.84% | 0.97%\n",
    "QDA | Penguin | 125 | [1/3, 1/3, 1/3] | 100 | 0.88% | 0.88%\n",
    "QDA | Penguin | 125 | [0.9, 0.05, 0.05] | 100 | 1.75% | 1.75%\n",
    "QDA | Penguin | 125 | [0.05, 0.9, 0.05] | 100 | 3.51% | 3.51%\n",
    "QDA | Penguin | 125 | [0.05, 0.05, 0.9] | 100 | 0.88% | 0.88%\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "En la tabla de arriba vemos la comparativa de entrenar un modelo QDA sobre el dataset *iris* y  *penguin* utilizando distribuciones *a priori* (ver columna *a_priori*) y con splt 70-30 y sin split (indicado como 100).\n",
    "\n",
    "**- ¿Se observan diferencias?**\n",
    "\n",
    "Respuesta:\n",
    "* Errores en función de la distribución a priori:\n",
    "    - Las distribuciones a priori no uniformes ([0.9, 0.05, 0.05], [0.05, 0.9, 0.05], [0.05, 0.05, 0.9]) tienden a generar mayores errores en comparación con la distribución uniforme ([1/3, 1/3, 1/3]).\n",
    "    - Los errores son más significativos en las configuraciones en las que se favorece una clase minoritaria, especialmente en el caso del iris con la configuración [0.05, 0.9, 0.05].\n",
    "    \n",
    "* Errores entre los splits 70/30 y 100:\n",
    "    - Los errores de prueba en el split 70/30 suelen ser ligeramente mejores que en el caso del entrenamiento completo (100). Esto puede deberse a que el conjunto de prueba evalúa el modelo con datos nuevos, mientras que el 100 evalúa sobre el mismo conjunto, donde es más propenso a errores causados por datos desbalanceados.\n",
    "    - La diferencia en errores es más marcada en el dataset iris que en penguin.\n",
    "\n",
    "**- ¿Por qué cree?**\n",
    "\n",
    "Respuesta:\n",
    "- **Impacto de la distribución a priori**:\n",
    "Cuando se usan distribuciones no uniformes, el modelo favorece una clase particular.\n",
    "\n",
    "- **Diferencias entre split 70/30 y 100**: En el caso 70/30, el conjunto de prueba representa una validación más realista del modelo. Como los datos están separados, se reduce el sobreajuste que ocurre al evaluar en el mismo conjunto usado para entrenamiento (caso 100).\n",
    "Sin embargo, en conjuntos más balanceados o grandes como penguin, la diferencia entre split 70/30 y 100 es menos significativa, ya que las distribuciones en el conjunto completo y en los subconjuntos son más parecidas.\n",
    "\n",
    "- **Propiedades de los datasets**:\n",
    "El dataset iris tiene menos muestras y podría ser más sensible al desbalance generado por distribuciones a priori no uniformes.\n",
    "Penguin tiene más datos y mejor balance, lo que amortigua el impacto de las decisiones sobre las distribuciones a priori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "3. Implementar el modelo **LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(BaseBayesianClassifier):\n",
    "    def _fit_params(self, X, y):\n",
    "        y_flat = y.flatten()\n",
    "        self.shared_cov = np.cov(X, bias=True)\n",
    "        self.inv_shared_cov = np.linalg.inv(self.shared_cov)    \n",
    "\n",
    "        self.means = [\n",
    "            X[:, y_flat == idx].mean(axis=1, keepdims=True)\n",
    "            for idx in range(len(self.log_a_priori))\n",
    "        ]\n",
    "    \"\"\"\n",
    "    Este método calcula el logaritmo de la probabilidad condicional para una observación x dada y una clase j.\n",
    "    \"\"\"\n",
    "    def _predict_log_conditional(self, x, class_idx):\n",
    "        unbiased_x = x - self.means[class_idx]\n",
    "        return -0.5 * unbiased_x.T @ self.inv_shared_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "3. Comparar **LDA** contra los mismos sets que **QDA** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA + Uniforme (cada clase tiene probabilidad 1/3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* LDA: Train (apparent) error is 0.0146 while test error is 0.0146\n",
      "Error (train): 1.46%\n",
      "Error (test): 1.46%\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "lda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])\n",
    "\n",
    "y_train_pred = lda.predict(train_x)\n",
    "y_test_pred = lda.predict(test_x)\n",
    "lda_train_acc = accuracy(train_y, y_train_pred)\n",
    "lda_test_acc = accuracy(test_y, y_test_pred)\n",
    "print(f\"* LDA: Train (apparent) error is {1-lda_train_acc:.4f} while test error is {1-lda_test_acc:.4f}\")\n",
    "\n",
    "# Calculate train and test errors\n",
    "error_train = np.mean(y_train_pred != train_y)\n",
    "error_test = np.mean(y_test_pred != test_y)\n",
    "\n",
    "print(f\"Error (train): {error_train:.2%}\")\n",
    "print(f\"Error (test): {error_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA + Primera clase con probabilidad 0.9, las demás 0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* LDA: Train (apparent) error is 0.4293 while test error is 0.4526\n",
      "Error (train): 42.93%\n",
      "Error (test): 45.26%\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "# Primera clase con probabilidad 0.9, las demás 0.05\n",
    "lda.fit(train_x, train_y, a_priori=[0.9, 0.05,  0.05])\n",
    "\n",
    "y_train_pred = lda.predict(train_x)\n",
    "y_test_pred = lda.predict(test_x)\n",
    "lda_train_acc = accuracy(train_y, y_train_pred)\n",
    "lda_test_acc = accuracy(test_y, y_test_pred)\n",
    "print(f\"* LDA: Train (apparent) error is {1-lda_train_acc:.4f} while test error is {1-lda_test_acc:.4f}\")\n",
    "\n",
    "# Calculate train and test errors\n",
    "error_train = np.mean(y_train_pred != train_y)\n",
    "error_test = np.mean(y_test_pred != test_y)\n",
    "\n",
    "print(f\"Error (train): {error_train:.2%}\")\n",
    "print(f\"Error (test): {error_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA + Segunda clase con probabilidad 0.9, las demás 0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* LDA: Train (apparent) error is 0.4000 while test error is 0.3796\n",
      "Error (train): 40.00%\n",
      "Error (test): 37.96%\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "# Segunda clase con probabilidad 0.9, las demás 0.05\n",
    "lda.fit(train_x, train_y, a_priori=[0.05, 0.9,  0.05])\n",
    "\n",
    "y_train_pred = lda.predict(train_x)\n",
    "y_test_pred = lda.predict(test_x)\n",
    "lda_train_acc = accuracy(train_y, y_train_pred)\n",
    "lda_test_acc = accuracy(test_y, y_test_pred)\n",
    "print(f\"* LDA: Train (apparent) error is {1-lda_train_acc:.4f} while test error is {1-lda_test_acc:.4f}\")\n",
    "\n",
    "# Calculate train and test errors\n",
    "error_train = np.mean(y_train_pred != train_y)\n",
    "error_test = np.mean(y_test_pred != test_y)\n",
    "\n",
    "print(f\"Error (train): {error_train:.2%}\")\n",
    "print(f\"Error (test): {error_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA + Tercera clase con probabilidad 0.9, las demás 0.05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* LDA: Train (apparent) error is 0.4293 while test error is 0.4672\n",
      "Error (train): 42.93%\n",
      "Error (test): 46.72%\n"
     ]
    }
   ],
   "source": [
    "lda = LDA()\n",
    "# Tercera clase con probabilidad 0.9, las demás 0.05\n",
    "lda.fit(train_x, train_y, a_priori=[0.05, 0.05,  0.9])\n",
    "\n",
    "y_train_pred = lda.predict(train_x)\n",
    "y_test_pred = lda.predict(test_x)\n",
    "lda_train_acc = accuracy(train_y, y_train_pred)\n",
    "lda_test_acc = accuracy(test_y, y_test_pred)\n",
    "print(f\"* LDA: Train (apparent) error is {1-lda_train_acc:.4f} while test error is {1-lda_test_acc:.4f}\")\n",
    "\n",
    "# Calculate train and test errors\n",
    "error_train = np.mean(y_train_pred != train_y)\n",
    "error_test = np.mean(y_test_pred != test_y)\n",
    "\n",
    "print(f\"Error (train): {error_train:.2%}\")\n",
    "print(f\"Error (test): {error_test:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabla Comparativa LDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo | Dataset | Seed | a_priori | Split | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "LDA | Iris | 125 | [1/3, 1/3, 1/3] | 70/30 | 12.38% | 13.33%\n",
    "LDA | Iris | 125 | [0.9, 0.05, 0.05] | 70/30 | 54.29% | 46.67%\n",
    "LDA | Iris | 125 | [0.05, 0.9, 0.05] | 70/30 | 63.81% | 55.56%\n",
    "LDA | Iris | 125 | [0.05, 0.05, 0.9] | 70/30 | 53.33% | 48.89%\n",
    "LDA | Iris | 125 | [1/3, 1/3, 1/3] | 100 | 13.33% | 13.33%\n",
    "LDA | Iris | 125 | [0.9, 0.05, 0.05] | 100 | 51.33% | 51.33%\n",
    "LDA | Iris | 125 | [0.05, 0.9, 0.05] | 100 | 62.67% | 62.67%\n",
    "LDA | Iris | 125 | [0.05, 0.05, 0.9] | 100 | 55.33% | 55.33%\n",
    " |  |  |  |  |  | \n",
    "LDA | Penguin | 125 | [1/3, 1/3, 1/3] | 70/30 | 0.84% | 0.97% \n",
    "LDA | Penguin | 125 | [0.9, 0.05, 0.05] | 70/30 | 44.77% | 37.86%\n",
    "LDA | Penguin | 125 | [0.05, 0.9, 0.05] | 70/30 | 38.49% | 44.66%\n",
    "LDA | Penguin | 125 | [0.05, 0.05, 0.9] | 70/30 | 45.19% | 39.81%\n",
    "LDA | Penguin | 125 | [1/3, 1/3, 1/3] | 100 | 0.88% | 0.88%\n",
    "LDA | Penguin | 125 | [0.9, 0.05, 0.05] | 100 | 42.98% | 42.98%\n",
    "LDA | Penguin | 125 | [0.05, 0.9, 0.05] | 100 | 40.64% | 40.64%\n",
    "LDA | Penguin | 125 | [0.05, 0.05, 0.9] | 100 | 43.57% | 43.57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Se observan diferencias con respecto a la tabla anterior de QDA?**\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "Sí, hay diferencias claras entre los resultados de LDA y QDA:\n",
    "- Errores significativamente mayores en LDA con distribuciones a priori no uniformes:\n",
    "    - En el dataset iris, los errores de LDA aumentan considerablemente para distribuciones no uniformes como [0.9, 0.05, 0.05] o [0.05, 0.9, 0.05], superando el 50% en algunos casos.\n",
    "    - En el dataset penguin, los errores de LDA también se incrementan notablemente con distribuciones no uniformes\n",
    "\n",
    "- Errores con distribuciones uniformes [1/3, 1/3, 1/3]:\n",
    "    - Para distribuciones uniformes, ambos modelos presentan errores bajos, aunque LDA tiene mayores errores que QDA en el dataset iris (13% vs 2%).\n",
    "\n",
    "- Impacto del split (70/30 vs 100):\n",
    "    - Para LDA, los errores entre el split 70/30 y 100 son similares. En QDA, el split 70/30 suele mostrar mejores resultados en el conjunto de prueba, especialmente en iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?** \n",
    "\n",
    "Respuesta:\n",
    "\n",
    "Sí, QDA es notoriamente mejor que LDA en este contexto, especialmente con distribuciones a priori no uniformes.\n",
    "- QDA es más robusto frente a distribuciones a priori no uniformes y datasets desbalanceados.\n",
    "- LDA puede ser más rápido y eficiente en casos donde las clases sean balanceadas y las distribuciones sean similares, pero es menos flexible que QDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la comparacion con Seeds de 125, 64 y 16. En todos los casos el split es 70-30 t la distribucion a priori es 1/3 para todas las clases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabla Comparativa LDA y QDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo | Dataset | Seed | a_priori | Split | Error (train) | Error (test)\n",
    ":---: | :---: | :---: | :---: | :---: | :---: | :---:\n",
    "QDA | Iris | 125 | [1/3, 1/3, 1/3] | 70/30 | 2.86% | 0.00%\n",
    "QDA | Iris | 64 | [1/3, 1/3, 1/3] | 70/30 | 0.00% | 4.44%   \n",
    "QDA | Iris | 16 |  [1/3, 1/3, 1/3] | 70/30 | 3.81% |2.22%   \n",
    "QDA | Penguin | 125 | [1/3, 1/3, 1/3] | 70/30 | 0.84% | 0.97%\n",
    "QDA | Penguin | 64 | [1/3, 1/3, 1/3] | 70/30 | 0.84% | 1.94%\n",
    "QDA | Penguin | 16 |  [1/3, 1/3, 1/3] | 70/30 | 1.26% | 0.97%\n",
    "|  |  |  |  |  | \n",
    "LDA | Iris | 125 | [1/3, 1/3, 1/3] | 70/30 | 12.38% | 13.33%\n",
    "LDA | Iris | 64 | [1/3, 1/3, 1/3] | 70/30 | 14.29% | 11.11% \n",
    "LDA | Iris | 16 |  [1/3, 1/3, 1/3] | 70/30 | 10.48% | 20.00%\n",
    "LDA | Penguin | 125 | [1/3, 1/3, 1/3] | 70/30 | 0.84% | 0.97%\n",
    "LDA | Penguin | 64 | [1/3, 1/3, 1/3] | 70/30 | 0.84% | 0.97%\n",
    "LDA | Penguin | 16 |  [1/3, 1/3, 1/3] | 70/30 | 1.26% | 0.00%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Se mantienen las conclusiones previas?\n",
    "Las conclusiones se matizan al considerar los nuevos resultados con diferentes random seeds:\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "- Errores en QDA:\n",
    "    - `Dataset iris`:\n",
    "    El error de entrenamiento y prueba varía con el random seed, pero en general, los errores son bajos.\n",
    "    Con el seed 64, el error de prueba aumenta ligeramente (4.44%), lo que podría atribuirse a la naturaleza aleatoria del split y a una posible menor representatividad de las clases en el conjunto de prueba.\n",
    "    - `Dataset penguin`:\n",
    "    Los errores son consistentemente bajos, tanto en entrenamiento como en prueba, con pequeñas variaciones que no afectan la conclusión de que QDA es un modelo efectivo para este dataset.\n",
    "\n",
    "- Errores en LDA:\n",
    "    - `Dataset iris`:\n",
    "    Los errores de entrenamiento y prueba también muestran variaciones según el random seed, con errores de prueba que van desde 11.11% hasta 20.00%.\n",
    "    Aunque los errores de LDA son mayores que los de QDA, el rango de variación no es dramático y sigue reflejando su menor capacidad para modelar la complejidad del dataset en comparación con QDA.\n",
    "    - `Dataset penguin`:\n",
    "    Al igual que QDA, LDA tiene errores consistentemente bajos, con variaciones mínimas. De hecho, con el seed 16, LDA tiene un error de prueba del 0%, lo que sugiere que el modelo fue particularmente efectivo en ese split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Estimar los tiempos de predicción de `TensorizedQDA`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tensorized_qda = TensorizedQDA()\n",
    "tensorized_qda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.55 ms ± 6.78 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tensorized_qda.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda = QDA()\n",
    "qda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.01 ms ± 24.2 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "qda.predict(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo | Dataset | Seed | a_priori | Split | Performance \n",
    ":---: | :---: | :---: | :---: | :---: | :---: \n",
    "Tensorized QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 2.7 ms ± 112 μs \n",
    "QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 7.23 ms ± 146 μs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "**De haber diferencias ¿Cuáles pueden ser las causas?**\n",
    "\n",
    "Respuesta:\n",
    "- El Tensorized QDA performa ~2.7 veces mas rapido que el QDA y esto se debe a que cuenta con una implementacion mas optimizada de la funcion `_predict_log_conditionals`\n",
    "- El mejor desempeño de `TensorizedQDA` se debe principalmente a la vectorización y al uso eficiente de operaciones en batch, que minimizan la latencia y el overhead computacional, mientras que QDA realiza los cálculos de manera iterativa y menos eficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización matemática"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "**Faster QDA**\n",
    "\n",
    "1. Para implementar FasterQDA, heredando de TensorizedQDA, nuestro objetivo es eliminar el ciclo for del método predict. Esto se logrará aprovechando operaciones matriciales vectorizadas y ajustando adecuadamente las dimensiones de los tensores mediante métodos como reshape, transpose, o flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    def _predict_log_conditionals(self, X):\n",
    "        \"\"\"\n",
    "        Calcula log(P(X|G)) para todos los datos y clases de manera vectorizada.\n",
    "        \"\"\"\n",
    "        # Expandir X para alinearlo con tensor_means\n",
    "        unbiased_X = X[np.newaxis, :, :] - self.tensor_means  # (n_classes, n_features, n_samples)\n",
    "\n",
    "        # Reorganizar para aplicar la matriz de covarianza inversa\n",
    "        unbiased_X = unbiased_X.transpose(0, 2, 1)  # (n_classes, n_samples, n_features)\n",
    "\n",
    "        # Producto interno: x^T * inv_cov * x\n",
    "        inner_prod = np.einsum(\n",
    "            'ijk,ikl,ijl->ij', unbiased_X, self.tensor_inv_cov, unbiased_X\n",
    "        )  # (n_classes, n_samples)\n",
    "\n",
    "        # Calcular logaritmo de probabilidad condicional\n",
    "        log_conditional = (\n",
    "            0.5 * np.log(np.linalg.det(self.tensor_inv_cov))[:, np.newaxis] - 0.5 * inner_prod\n",
    "        )  # (n_classes, n_samples)\n",
    "\n",
    "        return log_conditional\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice las clases de todas las muestras de manera vectorizada.\n",
    "        \"\"\"\n",
    "        # Calcular log(P(G)) + log(P(X|G))\n",
    "        log_posteriors = self.log_a_priori[:, np.newaxis] + self._predict_log_conditionals(X)\n",
    "\n",
    "        # Retornar la clase con máxima probabilidad para cada muestra\n",
    "        return np.argmax(log_posteriors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_qda = FasterQDA()\n",
    "faster_qda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.3 μs ± 2.9 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "faster_qda.predict(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`.\n",
    "De la tabla comparativa de mas abajo vemos que el `FasterQDA` es el mas rapido de los tres.\n",
    "\n",
    "\n",
    "Modelo | Dataset | Seed | a_priori | Split | Performance \n",
    ":---: | :---: | :---: | :---: | :---: | :---: \n",
    "Faster QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 65.9 μs ± 154 ns\n",
    "Tensorized QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 2.7 ms ± 112 μs \n",
    "QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 7.23 ms ± 146 μs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "3. Mostrar (puede ser con un print) dónde aparece la mencionada matriz de *n x n*, donde *n* es la cantidad de observaciones a predecir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Dónde podría generarse una matriz 𝑛×𝑛? Si accidentalmente calculáramos todos los productos cruzados entre muestras (por ejemplo, usando @ con ejes incorrectos o expandiendo dimensiones de forma innecesaria), generaríamos una matriz \n",
    "𝑛×𝑛."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos insertar un print en el lugar donde calculamos `inner_prod` y revisar las dimensiones de cada operación intermedia. Agreguemos el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    def _predict_log_conditionals(self, X):\n",
    "        \"\"\"\n",
    "        Calcula log(P(X|G)) para todos los datos y clases de manera vectorizada.\n",
    "        \"\"\"\n",
    "        unbiased_X = X[np.newaxis, :, :] - self.tensor_means  # (n_classes, n_features, n_samples)\n",
    "        unbiased_X = unbiased_X.transpose(0, 2, 1)  # (n_classes, n_samples, n_features)\n",
    "\n",
    "        print(\"unbiased_X shape:\", unbiased_X.shape)  # Dimensiones de unbiased_X\n",
    "        print(\"tensor_inv_cov shape:\", self.tensor_inv_cov.shape)  # Dimensiones de tensor_inv_cov\n",
    "\n",
    "        # Calcular producto interno\n",
    "        inner_prod = np.einsum(\n",
    "            'ijk,ikl,ijl->ij', unbiased_X, self.tensor_inv_cov, unbiased_X\n",
    "        )\n",
    "\n",
    "        print(\"inner_prod shape:\", inner_prod.shape)  # Debemos asegurarnos que sea (n_classes, n_samples)\n",
    "\n",
    "        log_conditional = (\n",
    "            0.5 * np.log(np.linalg.det(self.tensor_inv_cov))[:, np.newaxis] - 0.5 * inner_prod\n",
    "        )\n",
    "\n",
    "        return log_conditional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "4. Demostrar que $diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Pregunta 1'](optimization_mat_1.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Pregunta 1'](optimization_mat_2.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "5. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente. ¿Hay cambios en los tiempos de predicción?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementación FasterQDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    def _predict_log_conditionals(self, X):\n",
    "        \"\"\"\n",
    "        Calcula log(P(X|G)) para todas las clases de manera eficiente usando matrices n x p.\n",
    "        \"\"\"\n",
    "        # Expandir X para alinearlo con self.tensor_means\n",
    "        unbiased_X = X[np.newaxis, :, :] - self.tensor_means  # (n_classes, n_features, n_samples)\n",
    "\n",
    "        # Cálculo eficiente de x^T * inv_cov * x para cada muestra y clase\n",
    "        inner_prod = np.sum(\n",
    "            (unbiased_X.transpose(0, 2, 1) @ self.tensor_inv_cov) * unbiased_X.transpose(0, 2, 1),\n",
    "            axis=2,\n",
    "        )  # (n_classes, n_samples)\n",
    "\n",
    "        # Calcular log-condicional\n",
    "        log_conditional = (\n",
    "            0.5 * np.log(np.linalg.det(self.tensor_inv_cov))[:, np.newaxis] - 0.5 * inner_prod\n",
    "        )  # (n_classes, n_samples)\n",
    "\n",
    "        return log_conditional\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice las clases de todas las muestras de manera eficiente.\n",
    "        \"\"\"\n",
    "        # Calcular log(P(G)) + log(P(X|G))\n",
    "        log_posteriors = self.log_a_priori[:, np.newaxis] + self._predict_log_conditionals(X)\n",
    "\n",
    "        # Retornar la clase con máxima probabilidad para cada muestra\n",
    "        return np.argmax(log_posteriors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_faster_qda = FasterQDA()\n",
    "optimized_faster_qda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.1 μs ± 195 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "optimized_faster_qda.predict(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`.\n",
    "\n",
    "De la tabla comparativa de mas abajo vemos que el `Optimizado FasterQDA` es el mas rapido de los cuatros en el orden de los ~`28.2 μs` mientras que el mas lento `QDA` esta en los ~`7.23 ms`\n",
    "\n",
    "\n",
    "Modelo | Dataset | Seed | a_priori | Split | Performance \n",
    ":---: | :---: | :---: | :---: | :---: | :---: \n",
    "Optimizado Faster QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 28.2 μs ± 219 ns\n",
    "Faster QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 65.9 μs ± 154 ns\n",
    "Tensorized QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 2.7 ms ± 112 μs \n",
    "QDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 7.23 ms ± 146 μs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorizar LDA \n",
    "\n",
    "1. \"Tensorizar\" el modelo LDA y comparar sus tiempos de predicción con el modelo antes implementado. *Notar que, en modo tensorizado, se puede directamente precomputar $\\mu^T \\cdot \\Sigma^{-1} \\in \\mathbb{R}^{k \\times 1 \\times p}$ y guardar eso en vez de $\\Sigma^{-1}$.*\n",
    "2. LDA no sufre del problema antes descrito de QDA debido a que no computa productos internos, por lo que no tiene un verdadero costo extra en memoria predecir \"en batch\". Implementar el modelo `FasterLDA` y comparar sus tiempos de predicción con las versiones anteriores de LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorizedLDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorizedLDA(LDA):\n",
    "    def _fit_params(self, X, y):\n",
    "        super()._fit_params(X, y)  # Usa la lógica original de LDA para calcular shared_cov, inv_shared_cov y means.\n",
    "\n",
    "        self.tensor_means = np.stack(self.means, axis=0)\n",
    "\n",
    "        self.means_inv_cov = self.tensor_means.transpose(0, 2, 1) @ self.inv_shared_cov[np.newaxis, :, :]\n",
    "\n",
    "    def _predict_log_conditionals(self, X):\n",
    "        unbiased_X = X[np.newaxis, :, :] - self.tensor_means  # (k, p, n)\n",
    "        quadratic_term = np.einsum('kpn,pm,kpn->kn', unbiased_X, self.inv_shared_cov, unbiased_X)\n",
    "\n",
    "        log_conditionals = -0.5 * quadratic_term  # (k, n)\n",
    "        return log_conditionals\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_posteriors = self.log_a_priori[:, np.newaxis] + self._predict_log_conditionals(X)\n",
    "\n",
    "        return np.argmax(log_posteriors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorized_lda = TensorizedLDA()\n",
    "tensorized_lda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.5 μs ± 15.6 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tensorized_lda.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA()\n",
    "lda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.51 ms ± 313 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "lda.predict(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la tabla abajo vemos que claramente el Tensorized LDA performa muy superior al LDA unas 26 veces mas rapido (~  2.23 ms/86.2 μs).\n",
    "\n",
    "Modelo | Dataset | Seed | a_priori | Split | Performance \n",
    ":---: | :---: | :---: | :---: | :---: | :---: \n",
    "Tensorized LDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 86.2 μs ± 2.98 μs \n",
    "LDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 2.23 ms ± 5.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FasterLDA**\n",
    "\n",
    "2. LDA no sufre del problema antes descrito de QDA debido a que no computa productos internos, por lo que no tiene un verdadero costo extra en memoria predecir \"en batch\". Implementar el modelo `FasterLDA` y comparar sus tiempos de predicción con las versiones anteriores de LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para implementar el modelo FasterLDA, aprovecharemos el hecho de que LDA no tiene el problema del almacenamiento de grandes matrices intermedias, ya que no requiere calcular productos internos para cada observación y clase. Esto significa que podemos optimizar el cálculo de las probabilidades condicionales al precomputar operaciones clave.\n",
    "\n",
    "Implementación de FasterLDA\n",
    "El modelo `FasterLDA` precomputará eficientemente \n",
    "𝜇\n",
    "𝑇\n",
    "Σ^−1\n",
    "y \n",
    "−\n",
    "0.5\n",
    "𝜇\n",
    "𝑇\n",
    "Σ^−1\n",
    "𝜇\n",
    " μ durante el ajuste para evitar cálculos redundantes en predicciones.\n",
    "\n",
    "Discusión:\n",
    "La implementación de FasterLDA aprovecha la naturaleza lineal de las operaciones de LDA y es adecuada para escenarios de predicción \"en batch\". Aunque LDA ya es eficiente, esta optimización puede ser útil para grandes conjuntos de datos, reduciendo el tiempo computacional de predicción significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterLDA(LDA):\n",
    "    def _fit_params(self, X, y):\n",
    "        \"\"\"\n",
    "        Ajusta los parámetros del modelo LDA de forma optimizada.\n",
    "        \"\"\"\n",
    "        super()._fit_params(X, y)  # Usar la implementación original para obtener inv_shared_cov y means\n",
    "\n",
    "        # Precomputar mu^T @ Sigma^-1 (k, 1, p)\n",
    "        self.means_inv_cov = np.array([mu.T @ self.inv_shared_cov for mu in self.means])  # (k, 1, p)\n",
    "\n",
    "        # Precomputar -0.5 * mu^T @ Sigma^-1 @ mu (k,)\n",
    "        self.quadratic_means = np.array([-0.5 * (mu.T @ self.inv_shared_cov @ mu).flatten() for mu in self.means])  # (k,)\n",
    "\n",
    "    def _predict_log_conditionals(self, X):\n",
    "        \"\"\"\n",
    "        Calcula log(P(X|G)) de manera optimizada.\n",
    "        \"\"\"\n",
    "        # X tiene forma (p, n)\n",
    "        linear_term = (self.means_inv_cov @ X).squeeze(axis=1)  # (k, n)\n",
    "        log_conditionals = self.quadratic_means[:, np.newaxis] + linear_term  # (k, n)\n",
    "        return log_conditionals\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice las clases de las observaciones X usando el modelo optimizado.\n",
    "        \"\"\"\n",
    "        log_posteriors = self.log_a_priori[:, np.newaxis] + self._predict_log_conditionals(X)\n",
    "        return np.argmax(log_posteriors, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_lda = FasterLDA()\n",
    "faster_lda.fit(train_x, train_y, a_priori=[0.33333333, 0.33333333,  0.33333333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.8 μs ± 2.12 μs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "faster_lda.predict(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la tabla abajo vemos que claramente el `Faster LDA` performa superior al `Tensorized LDA` unas 7 veces mas rapido (~  86.2 μs / 11.7 μs).\n",
    "\n",
    "Modelo | Dataset | Seed | a_priori | Split | Performance \n",
    ":---: | :---: | :---: | :---: | :---: | :---: \n",
    "Faster LDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 11.7 μs ± 56.3 ns \n",
    "Tensorized LDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 86.2 μs ± 2.98 μs \n",
    "LDA | Penguins | 125 | [1/3, 1/3, 1/3] | 70/30 | 2.23 ms ± 5.53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preguntas Teoricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Pregunta 1'](pregunta_teorica_1.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Pregunta 3'](pregunta_teorica_2.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Pregunta 3'](pregunta_teorica_3.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Pregunta 1'](ejercicio_teorico_1.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Pregunta 1'](ejercicio_teorico_2.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Pregunta 1'](ejercicio_teorico_3.jpg) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "intro_ai_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
